{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1b6f14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "import credenciais.credenciais_pedro_report as cpr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime as dt\n",
    "import datetime as datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df= pd.read_csv('df_no_duplicates.csv')\n",
    "df_n_dup = df.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20285eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primeiro bloco de código. Definição de o que é um Outlier.\n",
    "# Se quiserem adicionar ou criar algum outro critério, é só mudar esse bloco, acho que é o mais importante...\n",
    "# Vale a pena dar uma checada pra ver se esses valores realmente tão fazendo sentido pra chamar de outliers...\n",
    "# Dessa vez mais na prática\n",
    "\n",
    "l = []\n",
    "for i in df_n_dup['category_nm'].unique():\n",
    "    if df_n_dup[(df_n_dup[\"category_nm\"].isin([i]))&(df_n_dup[\"vendido\"] == 1)].dropna().shape[0] > 50:\n",
    "        collectn_1 = np.array(df_n_dup[(df_n_dup['category_nm'].isin([i]))&(df_n_dup['vendido'] == 1)].dropna()['2_1'])\n",
    "        collectn_2 = np.array(df_n_dup[(df_n_dup['category_nm'].isin([i]))&(df_n_dup['vendido'] == 1)].dropna()['3_1'])\n",
    "        collectn_3 = np.array(df_n_dup[(df_n_dup['category_nm'].isin([i]))&(df_n_dup['vendido'] == 1)].dropna()['1_1'])\n",
    "        collectn_4 = np.array(df_n_dup[(df_n_dup['category_nm'].isin([i]))&(df_n_dup['vendido'] == 1)].dropna()['densidade'])  \n",
    "        collectn_5 = np.array(df_n_dup[(df_n_dup['category_nm'].isin([i]))&(df_n_dup['vendido'] == 1)].dropna()['gross_weight_nu'])            \n",
    "\n",
    "        l.append([i,np.median(collectn_1),np.median(collectn_2),                      \n",
    "                  np.std(collectn_1),np.std(collectn_2),\n",
    "                  np.median(collectn_3),np.mean(collectn_3),\n",
    "                  np.std(collectn_3),\n",
    "                  (np.median(collectn_1) - np.std(collectn_1)*1.645, np.median(collectn_1) + np.std(collectn_1)*1.645),  \n",
    "                  (np.median(collectn_2) - np.std(collectn_2)*1.645, np.median(collectn_2) + np.std(collectn_2)*1.645),\n",
    "                  (np.median(collectn_3) - np.std(collectn_3)*1.645, np.median(collectn_3) + np.std(collectn_3)*1.645),\n",
    "                  (np.median(collectn_4) - np.std(collectn_4)*1.645, np.median(collectn_4) + np.std(collectn_4)*1.645),\n",
    "                  (np.median(collectn_5) - np.std(collectn_5)*1.645, np.median(collectn_5) + np.std(collectn_5)*1.645),\n",
    "                  len(collectn_3)])\n",
    "df_intervals = pd.DataFrame(l,columns=['Segment','1st','2nd','2nd_std','cfd_int1','Max_Median','Max_Mean','Max_SD','2_1','3_1','1_1','densidade','gross_weight_nu','Sample'])\\\n",
    "            .dropna().sort_values(by=['1st','2nd'],ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "248e858a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Juntando Outliers!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████▉| 292390/292391 [05:44<00:00, 849.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# Junção dos Outliers encontrados com base nos critérios definidos Acima. \n",
    "# Adição de uma coluna nova representando eles.\n",
    "\n",
    "\n",
    "dictionary = {'2_1':[],'3_1':[],'1_1':[],'densidade':[],'gross_weight_nu':[]}\n",
    "print('Juntando Outliers!')\n",
    "for i,v in zip(df_n_dup.iloc,tqdm(range(len(df_n_dup)))):\n",
    "    if list(df_intervals[df_intervals['Segment'] == i['category_nm']]['3_1']) != []:\n",
    "        for j in dictionary.keys():\n",
    "            dictionary[j].append(df_intervals[df_intervals['Segment'] == i['category_nm']][j].values[0][0] < i[j] <df_intervals[df_intervals['Segment'] == i['category_nm']][j].values[0][1])    \n",
    "    else:\n",
    "        for j in dictionary.keys():\n",
    "            dictionary[j].append('Pouca Amostra')\n",
    "for i in dictionary.keys():\n",
    "    df_n_dup[f'{i}_int'] = dictionary[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ef2bd5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Iniciando 4 CVs x 3 Modelos x 15 Combinações HiperParâmetros x 102 Categorias ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|████████████████████████████████████████▌| 101/102 [21:03<00:12, 12.51s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Categoria</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>Amostra</th>\n",
       "      <th>params_1</th>\n",
       "      <th>params_2</th>\n",
       "      <th>Modelo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Livros, Revistas e Comics &gt; Outros</td>\n",
       "      <td>309.990466</td>\n",
       "      <td>332197.707534</td>\n",
       "      <td>0.738586</td>\n",
       "      <td>285.25</td>\n",
       "      <td>75.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Gradient Boosting Regressor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brinquedos e Hobbies &gt; Brinquedos de Montar &gt; ...</td>\n",
       "      <td>209.393342</td>\n",
       "      <td>102295.094069</td>\n",
       "      <td>0.399364</td>\n",
       "      <td>400.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Random Forest Regressor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Casa, Móveis e Decoração &gt; Têxteis de Casa e D...</td>\n",
       "      <td>656.646123</td>\n",
       "      <td>753221.072067</td>\n",
       "      <td>0.513574</td>\n",
       "      <td>39.5</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Random Forest Regressor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Casa, Móveis e Decoração &gt; Organização para Casa</td>\n",
       "      <td>370.200281</td>\n",
       "      <td>240531.211324</td>\n",
       "      <td>0.60245</td>\n",
       "      <td>385.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Gradient Boosting Regressor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Casa, Móveis e Decoração &gt; Cozinha &gt; Louça e A...</td>\n",
       "      <td>189.563204</td>\n",
       "      <td>83797.659298</td>\n",
       "      <td>0.440932</td>\n",
       "      <td>227.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Gradient Boosting Regressor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Informática &gt; Conectividade e Redes &gt; Roteadores</td>\n",
       "      <td>186.788996</td>\n",
       "      <td>74674.998129</td>\n",
       "      <td>0.489468</td>\n",
       "      <td>84.75</td>\n",
       "      <td>115.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Random Forest Regressor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Construção &gt; Outros</td>\n",
       "      <td>311.26604</td>\n",
       "      <td>184411.298245</td>\n",
       "      <td>0.68729</td>\n",
       "      <td>465.5</td>\n",
       "      <td>125.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Gradient Boosting Regressor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Arte, Papelaria e Armarinho &gt; Outros</td>\n",
       "      <td>195.508548</td>\n",
       "      <td>82970.003332</td>\n",
       "      <td>0.646147</td>\n",
       "      <td>173.5</td>\n",
       "      <td>75.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Gradient Boosting Regressor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Construção &gt; Mobiliário para Banheiros &gt; Torne...</td>\n",
       "      <td>287.749746</td>\n",
       "      <td>172332.062706</td>\n",
       "      <td>0.378094</td>\n",
       "      <td>117.25</td>\n",
       "      <td>115.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Random Forest Regressor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Categoria         MAE  \\\n",
       "0                 Livros, Revistas e Comics > Outros  309.990466   \n",
       "1  Brinquedos e Hobbies > Brinquedos de Montar > ...  209.393342   \n",
       "2  Casa, Móveis e Decoração > Têxteis de Casa e D...  656.646123   \n",
       "3   Casa, Móveis e Decoração > Organização para Casa  370.200281   \n",
       "4  Casa, Móveis e Decoração > Cozinha > Louça e A...  189.563204   \n",
       "5   Informática > Conectividade e Redes > Roteadores  186.788996   \n",
       "6                                Construção > Outros   311.26604   \n",
       "7               Arte, Papelaria e Armarinho > Outros  195.508548   \n",
       "8  Construção > Mobiliário para Banheiros > Torne...  287.749746   \n",
       "\n",
       "             MSE      MAPE Amostra params_1 params_2  \\\n",
       "0  332197.707534  0.738586  285.25     75.0      2.0   \n",
       "1  102295.094069  0.399364   400.0    125.0      8.0   \n",
       "2  753221.072067  0.513574    39.5    100.0      2.0   \n",
       "3  240531.211324   0.60245   385.0    115.0      8.0   \n",
       "4   83797.659298  0.440932   227.0     75.0      8.0   \n",
       "5   74674.998129  0.489468   84.75    115.0      4.0   \n",
       "6  184411.298245   0.68729   465.5    125.0      8.0   \n",
       "7   82970.003332  0.646147   173.5     75.0      4.0   \n",
       "8  172332.062706  0.378094  117.25    115.0      2.0   \n",
       "\n",
       "                        Modelo  \n",
       "0  Gradient Boosting Regressor  \n",
       "1      Random Forest Regressor  \n",
       "2      Random Forest Regressor  \n",
       "3  Gradient Boosting Regressor  \n",
       "4  Gradient Boosting Regressor  \n",
       "5      Random Forest Regressor  \n",
       "6  Gradient Boosting Regressor  \n",
       "7  Gradient Boosting Regressor  \n",
       "8      Random Forest Regressor  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('-'*15,'Iniciando 4 CVs x 3 Modelos x 15 Combinações HiperParâmetros x 102 Categorias', '-'*15)\n",
    "\n",
    "# Fase 1 --> criar todas as combinações possíveis de regressores com seus respectivos hiperparâmetros\n",
    "\n",
    "modelos = {'Random Forest Regressor':{},\n",
    "          'Gradient Boosting Regressor':{},\n",
    "          'MLP Regressor': {}}\n",
    "\n",
    "for n_estimatorRF, n_estimatorGB, hidden_layers in zip([75,85,100,115,125],[75,85,100,115,125],[75,85,100,125,150]):\n",
    "    for min_sample_splitRF,min_sample_splitGB,learning_rate in zip([2,4,8],[2,4,8],[0.00075,0.001,0.002]):\n",
    "        modelos['Random Forest Regressor']\\\n",
    "        .update({f'params_1: {n_estimatorRF} - params_2: {min_sample_splitRF}':\n",
    "                RandomForestRegressor(n_estimators=n_estimatorRF,min_samples_split=min_sample_splitRF)})\n",
    "        modelos['Gradient Boosting Regressor']\\\n",
    "        .update({f'params_1: {n_estimatorGB} - params_2: {min_sample_splitGB}':\n",
    "                GradientBoostingRegressor(n_estimators=n_estimatorGB,min_samples_split=min_sample_splitGB)})        \n",
    "        modelos['MLP Regressor']\\\n",
    "        .update({f'params_1: {hidden_layers} - params_2: {learning_rate}':\n",
    "                MLPRegressor(hidden_layer_sizes=hidden_layers,learning_rate_init=learning_rate)})        \n",
    "\n",
    "total = pd.DataFrame({'MAE':[],'MSE':[],'MAPE':[],'Amostra':[]})\n",
    "\n",
    "# Primeiro Loop, verificando categoria por categoria\n",
    "\n",
    "for i,v in zip(df_n_dup[df_n_dup['2_1_int'] != 'Pouca Amostra']['category_nm'].unique(),tqdm(range(len(df_n_dup[df_n_dup['2_1_int'] != 'Pouca Amostra']['category_nm'].unique())))):\n",
    "    try:\n",
    "        df_testing = df_n_dup[(df_n_dup['category_nm'] == i)&\n",
    "                             (df_n_dup['gross_weight_nu_int'] == 1)&(df_n_dup['3_1_int'] == 1)&\n",
    "                             (df_n_dup['2_1_int'] == 1)&(df_n_dup['1_1_int'] == 1)&(df_n_dup['densidade_int'] == 1)\n",
    "                            ]\n",
    "\n",
    "        df_modelo = df_testing[['brand_nm','3_1','2_1','1_1','volume','gross_weight_nu']]\n",
    "        df_modelo = pd.concat([df_modelo.drop(columns='brand_nm'), pd.get_dummies(df_modelo['brand_nm'])],1)\n",
    "\n",
    "\n",
    "        dictionary_models = {}\n",
    "        partitions = KFold(n_splits=4)\n",
    "        \n",
    "        #Terceiro Loop, verificando Cross-Validation por Cross-Validation\n",
    "\n",
    "        for train,test in partitions.split(df_modelo):\n",
    "            X_test, y_test = df_modelo.iloc[test].drop(columns='gross_weight_nu'), df_modelo.iloc[test][['gross_weight_nu']]\n",
    "            X_train, y_train = df_modelo.iloc[train].drop(columns='gross_weight_nu'), df_modelo.iloc[train][['gross_weight_nu']]\n",
    "            dictionary_models[f'{test[0]} - {test[-1]}'] = {}\n",
    "            \n",
    "            # Quarto e Quinto Loop, Verificando Modelo por Modelo\n",
    "            \n",
    "            for modelo in modelos.keys():\n",
    "                for hiperparametros in modelos[modelo].keys():\n",
    "\n",
    "                    predicting = modelos[modelo][hiperparametros].fit(X_train,y_train)\n",
    "                    predictions = predicting.predict(X_test)\n",
    "\n",
    "                    dictionary_models[f'{test[0]} - {test[-1]}'].update({f'{modelo} - {hiperparametros}':{\n",
    "                      'MAE':mean_absolute_error(predictions,y_test),\n",
    "                      'MSE':mean_squared_error(predictions,y_test),\n",
    "                      'MAPE':mean_absolute_percentage_error(predictions,y_test),\n",
    "                      'Amostra':len(y_test)}})\n",
    "        \n",
    "        # Só colocando tudo em um formato mais \"visual\", ou seja, em um DataFrame\n",
    "        metricas ={'MAE':{},'MSE':{},'MAPE':{},'Amostra':{}}\n",
    "        for item,value in metricas.items():\n",
    "            models = {}\n",
    "            for j in dictionary_models[list(dictionary_models.keys())[0]].keys():\n",
    "                mean = []\n",
    "                for k in dictionary_models.keys():\n",
    "                    mean.append(dictionary_models[k][j][item])\n",
    "                models.update({j:np.mean(mean)})\n",
    "            metricas[item].update(models)\n",
    "        resultados_tuning = pd.DataFrame(metricas)\n",
    "        \n",
    "        # Adicionando os hiperparâmetros mais eficientes encontrados para cada categoria\n",
    "        \n",
    "        dictionary = {'params_1':[],'params_2':[]}\n",
    "        for k in resultados_tuning.index:\n",
    "            dictionary['params_1'].append(float(k.split(' - ')[1].split(': ')[1]))\n",
    "            dictionary['params_2'].append(float(k.split(' - ')[2].split(': ')[1]))\n",
    "        for k in dictionary.keys():\n",
    "            resultados_tuning[k] = dictionary[k]\n",
    "        \n",
    "        # \n",
    "        df_melhor = pd.DataFrame(resultados_tuning.sort_values(by=['MAPE','MAE']).iloc[0])\n",
    "        df_melhor.loc['Modelo'] = df_melhor.columns[0].split(' - ')[0]\n",
    "        total = pd.concat([total,df_melhor.rename(columns={df_melhor.columns[0]:i}).transpose()])\n",
    "    except: \n",
    "        pass\n",
    "total = total.rename_axis('Categoria').reset_index()    \n",
    "\n",
    "display(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2f340b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Juntando Estimativas!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|████████████████████████████████████████▌| 101/102 [00:39<00:00,  2.55it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sku_nm</th>\n",
       "      <th>brand_nm</th>\n",
       "      <th>3_1</th>\n",
       "      <th>2_1</th>\n",
       "      <th>1_1</th>\n",
       "      <th>volume</th>\n",
       "      <th>gross_weight_nu</th>\n",
       "      <th>Fix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bíblia Do Obreiro Com Letras Vermelhas Ra - Lu...</td>\n",
       "      <td>CLARO</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1764.0</td>\n",
       "      <td>76000.0</td>\n",
       "      <td>563.138890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Livro - Direito De Família - Madaleno</td>\n",
       "      <td>DIREITO</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1632.0</td>\n",
       "      <td>209000.0</td>\n",
       "      <td>601.974712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bíblia Da Mulher Vitoriosa Média - Letra Gigan...</td>\n",
       "      <td>VINHO</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1232.0</td>\n",
       "      <td>85000.0</td>\n",
       "      <td>554.567819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lego Creator - Motores De Corrida Radical - 31072</td>\n",
       "      <td>Lego</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>246.402504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LEGO Friends - Carrinho de Sorvetes - Stephani...</td>\n",
       "      <td>LEGO</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>5.0</td>\n",
       "      <td>840.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>94.345366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Papel Vegetal Filiperson A4 60G 10 Folhas Tran...</td>\n",
       "      <td>Filiperson</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1176.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>275.739982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>Lapis De Cor 36 Cores Faber Multicolor Super.</td>\n",
       "      <td>FABER-CASTELL</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>245.050532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Verniz Vitral Acrilex Preto</td>\n",
       "      <td>ACRILEX</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2268.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>214.499705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Tinta Acrilica Profissional Amarelo Pele 20ml ...</td>\n",
       "      <td>Acrilex</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>3.0</td>\n",
       "      <td>561.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>241.021034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>Tinta Acrilica Profissional Preto 20ml Acrilex</td>\n",
       "      <td>Acrilex</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>3.0</td>\n",
       "      <td>561.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>241.021034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1877 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sku_nm       brand_nm  \\\n",
       "0    Bíblia Do Obreiro Com Letras Vermelhas Ra - Lu...          CLARO   \n",
       "1                Livro - Direito De Família - Madaleno        DIREITO   \n",
       "2    Bíblia Da Mulher Vitoriosa Média - Letra Gigan...          VINHO   \n",
       "0    Lego Creator - Motores De Corrida Radical - 31072           Lego   \n",
       "1    LEGO Friends - Carrinho de Sorvetes - Stephani...           LEGO   \n",
       "..                                                 ...            ...   \n",
       "100  Papel Vegetal Filiperson A4 60G 10 Folhas Tran...     Filiperson   \n",
       "101      Lapis De Cor 36 Cores Faber Multicolor Super.  FABER-CASTELL   \n",
       "102                        Verniz Vitral Acrilex Preto        ACRILEX   \n",
       "103  Tinta Acrilica Profissional Amarelo Pele 20ml ...        Acrilex   \n",
       "104     Tinta Acrilica Profissional Preto 20ml Acrilex        Acrilex   \n",
       "\n",
       "          3_1       2_1  1_1  volume  gross_weight_nu         Fix  \n",
       "0    0.285714  0.666667  6.0  1764.0          76000.0  563.138890  \n",
       "1    0.166667  0.708333  4.0  1632.0         209000.0  601.974712  \n",
       "2    0.181818  0.636364  4.0  1232.0          85000.0  554.567819  \n",
       "0    0.250000  0.700000  5.0  1400.0           3000.0  246.402504  \n",
       "1    0.357143  0.857143  5.0   840.0           3000.0   94.345366  \n",
       "..        ...       ...  ...     ...              ...         ...  \n",
       "100  0.071429  0.750000  2.0  1176.0           3000.0  275.739982  \n",
       "101  0.100000  0.700000  2.0   560.0           3000.0  245.050532  \n",
       "102  0.500000  0.777778  9.0  2268.0           3000.0  214.499705  \n",
       "103  0.176471  0.647059  3.0   561.0           3000.0  241.021034  \n",
       "104  0.176471  0.647059  3.0   561.0           3000.0  241.021034  \n",
       "\n",
       "[1877 rows x 8 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Agora a gente vai usar o melhor modelo encontrado pra cada categoria.\n",
    "# A gente descobre isso no dataframe que a gente criou lá atrás...\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "FixFinalDF = pd.DataFrame({'sku_nm':[],'brand_nm':[],'3_1':[],'2_1':[],'1_1':[],'volume':[],'gross_weight_nu':[]})\n",
    "print('Juntando Estimativas!!')\n",
    "for i,v in zip(df_n_dup[df_n_dup['2_1_int'] != 'Pouca Amostra']['category_nm'].unique(),tqdm(range(len(df_n_dup[df_n_dup['2_1_int'] != 'Pouca Amostra']['category_nm'].unique())))):\n",
    "    try:\n",
    "        df_train = df_n_dup[(df_n_dup['category_nm'] == i)&\n",
    "                             (df_n_dup['gross_weight_nu_int'] == 1)&(df_n_dup['3_1_int'] == 1)&\n",
    "                             (df_n_dup['2_1_int'] == 1)&(df_n_dup['1_1_int'] == 1)&(df_n_dup['densidade_int'] == 1)\n",
    "                            ][\n",
    "                            ['brand_nm','3_1','2_1','1_1','volume','gross_weight_nu']]\n",
    "        dummies = pd.get_dummies(df_train['brand_nm'])\n",
    "        df_train = pd.concat([df_train.drop(columns='brand_nm'), dummies],1)\n",
    "\n",
    "        df_fixing = df_n_dup[(df_n_dup['category_nm'] == i)&\n",
    "                             (df_n_dup['gross_weight_nu_int'] == 0)&(df_n_dup['3_1_int'] == 1)&\n",
    "                             (df_n_dup['2_1_int'] == 1)&(df_n_dup['1_1_int'] == 1)&(df_n_dup['densidade_int'] == 1)\n",
    "                            ][\n",
    "                            ['sku_nm','brand_nm','3_1','2_1','1_1','volume','gross_weight_nu']]\n",
    "        \n",
    "        parametros = total[total['Categoria'] == i][['Modelo','params_1','params_2']]\n",
    "        \n",
    "        for i in dummies.columns:\n",
    "            l = []\n",
    "            for j in df_fixing['brand_nm']:\n",
    "                if j == i:\n",
    "                    l.append(1)\n",
    "                else:\n",
    "                    l.append(0)\n",
    "            df_fixing[i] = l\n",
    "        \n",
    "        \n",
    "        if parametros['Modelo'].values[0] == 'Random Forest Regressor':\n",
    "            predicting_final = RandomForestRegressor(n_estimators= int(parametros['params_1'].values[0]),\n",
    "                                                     min_samples_split = int(parametros['params_2'].values[0]))\\\n",
    "            .fit(df_train.drop(columns='gross_weight_nu'),df_train['gross_weight_nu'])\n",
    "        elif parametros['Modelo'].values[0] == 'Gradient Boosting Regressor':\n",
    "            predicting_final = GradientBoostingRegressor(n_estimators= int(parametros['params_1'].values[0]),\n",
    "                                                     min_samples_split = int(parametros['params_2'].values[0]))\\\n",
    "            .fit(df_train.drop(columns='gross_weight_nu'),df_train['gross_weight_nu'])\n",
    "        elif parametros['Modelo'].values[0] == 'MLP Regressor':\n",
    "            predicting_final = MLPRegressor(hidden_layer_sizes = parametros['params_1'].values[0],\n",
    "                                                          learning_rate_init = parametros['params_2'].values[0])\\\n",
    "                .fit(df_train.drop(columns='gross_weight_nu'),df_train['gross_weight_nu'])        \n",
    "        \n",
    "        predictions = predicting_final.predict(df_fixing.drop(columns=['gross_weight_nu','brand_nm','sku_nm']))\n",
    "\n",
    "        FixFinalDF = pd.concat([FixFinalDF,pd.concat([df_fixing[['sku_nm','brand_nm','3_1','2_1','1_1','volume','gross_weight_nu']].reset_index(drop=True)\n",
    "                   ,pd.DataFrame(predictions).rename(columns={0:'Fix'})],1)])\n",
    "    except:\n",
    "        pass\n",
    "FixFinalDF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
